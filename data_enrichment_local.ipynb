{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ezdO2Zp5ffNo"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas numpy tqdm\n",
        "# !pip install torch torchvision torchaudio\n",
        "# !pip install transformers sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "myim0uP5flJg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: True\n",
            "GPU model: NVIDIA GeForce RTX 3090 Ti\n",
            "\n",
            "‚úÖ Successfully loaded data with 7537 rows\n",
            "\n",
            "üìä Sample of your data:\n",
            "          title  date                                           question  \\\n",
            "0  Noam Chomsky  2017  How do you view your own contributions to the ...   \n",
            "1  Noam Chomsky  2017  What's your perspective on the Chomsky hierarc...   \n",
            "2  Noam Chomsky  2017  Could you elaborate on the concept of a univer...   \n",
            "\n",
            "                                              answer  \n",
            "0  Considered the founder of modern linguistics, ...  \n",
            "1  I introduced the Chomsky hierarchy, which has ...  \n",
            "2  The concept of a universal grammar underlies a...  \n",
            "\n",
            "‚úÖ All required columns present\n",
            "\n",
            "ü§ñ Loading FLAN-T5 Large model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Model loaded and running on GPU\n",
            "\n",
            "üîç Processing all rows to extract metadata...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7537 [00:00<?, ?it/s]/home/pdm/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/pdm/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 6285/7537 [2:29:14<27:53,  1.34s/it]  Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7537/7537 [2:55:08<00:00,  1.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Saving enriched data...\n",
            "\n",
            "‚úÖ Data saved to enriched_data_before.json and enriched_data_before.csv\n",
            "\n",
            "üì• The enriched data files have been saved to your current working directory.\n",
            "\n",
            "üìä Sample of enriched data:\n",
            "   year      setting              topic              persona  \\\n",
            "0  2017  MIT Lecture        Linguistics            Professor   \n",
            "1  2017   Radio Show         Philosophy  Cognitive Scientist   \n",
            "2  2017  MIT Lecture  Universal grammar  Cognitive Scientist   \n",
            "\n",
            "                                         instruction  \\\n",
            "0  Respond as a Professor in a MIT Lecture discus...   \n",
            "1  Respond as a Cognitive Scientist in a Radio Sh...   \n",
            "2  Respond as a Cognitive Scientist in a MIT Lect...   \n",
            "\n",
            "                                               input  \\\n",
            "0  How do you view your own contributions to the ...   \n",
            "1  What's your perspective on the Chomsky hierarc...   \n",
            "2  Could you elaborate on the concept of a univer...   \n",
            "\n",
            "                                              output  \n",
            "0  Considered the founder of modern linguistics, ...  \n",
            "1  I introduced the Chomsky hierarchy, which has ...  \n",
            "2  The concept of a universal grammar underlies a...  \n",
            "\n",
            "üéâ Processing complete! You can now use the generated JSON and CSV files from your local directory.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n# Process next 10 rows\\nmore_data = process_additional_data(10, 10)\\n\\n# Add to existing data\\nenriched_data.extend(more_data)\\n\\n# Save updated data\\nwith open(output_file_json, \\'w\\') as f:\\n    json.dump(enriched_data, f, indent=2)\\n\\n# Update CSV\\npd.DataFrame(enriched_data).to_csv(output_file_csv, index=False)\\nprint(f\"\\n‚úÖ Updated data saved to {output_file_json} and {output_file_csv}\")\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm  # Use `tqdm` for the terminal if needed\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Check for GPU availability\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Step 1: Load the CSV file\n",
        "file_name = \"cleaned_2018Before.csv\"\n",
        "df = pd.read_csv(file_name)\n",
        "print(f\"\\n‚úÖ Successfully loaded data with {len(df)} rows\")\n",
        "print(\"\\nüìä Sample of your data:\")\n",
        "print(df.head(3))  # Using print for local running\n",
        "\n",
        "# Verify required columns exist\n",
        "required_columns = ['title', 'date', 'question', 'answer']\n",
        "missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "if missing_columns:\n",
        "    print(f\"\\n‚ö†Ô∏è Warning: Missing required columns: {', '.join(missing_columns)}\")\n",
        "    print(\"Please ensure your CSV has 'title', 'date', 'question', and 'answer' columns.\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All required columns present\")\n",
        "\n",
        "# Step 3: Install and import necessary libraries\n",
        "# If you haven't installed the transformers and sentencepiece libraries, run the following in your terminal:\n",
        "# pip install transformers sentencepiece\n",
        "\n",
        "# Step 4: Initialize the FLAN-T5 Large model\n",
        "print(\"\\nü§ñ Loading FLAN-T5 Large model...\")\n",
        "model_name = \"google/flan-t5-large\"\n",
        "\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    # Create pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=0 if torch.cuda.is_available() else -1,\n",
        "        max_length=200\n",
        "    )\n",
        "    return pipe\n",
        "\n",
        "# Load model\n",
        "llm = load_model()\n",
        "print(f\"\\n‚úÖ Model loaded and running on {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# Step 5: Create functions for metadata extraction\n",
        "def extract_year(date_str):\n",
        "    \"\"\"Extract year from date string\"\"\"\n",
        "    try:\n",
        "        # Try to find a 4-digit number in the string\n",
        "        year_match = re.search(r'\\b(19|20)\\d{2}\\b', str(date_str))\n",
        "        if year_match:\n",
        "            return int(year_match.group(0))\n",
        "\n",
        "        # If no match found, return current year\n",
        "        import datetime\n",
        "        return datetime.datetime.now().year\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting year: {e}\")\n",
        "        return 2023  # Default year\n",
        "\n",
        "def get_llm_response(prompt, max_length=100, temperature=0.2, num_return_sequences=1):\n",
        "    \"\"\"Get response from FLAN-T5 model\"\"\"\n",
        "    try:\n",
        "        # Add retry mechanism for robustness\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                result = llm(\n",
        "                    prompt,\n",
        "                    max_length=max_length,\n",
        "                    temperature=temperature,\n",
        "                    num_return_sequences=num_return_sequences\n",
        "                )\n",
        "                return result[0]['generated_text'].strip()\n",
        "            except Exception as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"Retry {attempt+1}/{max_retries}: {e}\")\n",
        "                    time.sleep(2)  # Wait before retrying\n",
        "                else:\n",
        "                    raise e\n",
        "    except Exception as e:\n",
        "        print(f\"Error with LLM: {e}\")\n",
        "        return \"Could not generate response\"\n",
        "\n",
        "def determine_setting(title, question, answer):\n",
        "    \"\"\"Generate an appropriate setting for the content\"\"\"\n",
        "\n",
        "    # Extract meaningful content and truncate to reasonable lengths\n",
        "    title_short = title[:100] if title else \"\"\n",
        "    question_short = question[:200] if question else \"\"\n",
        "    answer_short = answer[:300] if answer else \"\"\n",
        "\n",
        "    # Create a detailed prompt\n",
        "    prompt = f\"\"\"\n",
        "    Task: Analyze the content and determine the most appropriate setting or format where this conversation or exchange would take place.\n",
        "\n",
        "    Examples of settings include:\n",
        "    - \"MIT Lecture\" (academic lecture at a university)\n",
        "    - \"Television Interview\" (TV show interview)\n",
        "    - \"Oxford Debate\" (formal debate setting)\n",
        "    - \"Panel Discussion\" (experts discussing at a conference)\n",
        "    - \"Political Rally\" (political speech or campaign event)\n",
        "    - \"Podcast Discussion\" (audio format conversation)\n",
        "    - \"Classroom Seminar\" (educational setting)\n",
        "    - \"Press Conference\" (official statements to media)\n",
        "    - \"Radio Show\" (audio broadcast)\n",
        "    - \"Online Forum\" (internet discussion)\n",
        "\n",
        "    Content to analyze:\n",
        "    TITLE: {title_short}\n",
        "    QUESTION: {question_short}\n",
        "    ANSWER EXCERPT: {answer_short}\n",
        "\n",
        "    Based on the tone, formality, style, and content, what is the most appropriate setting for this exchange?\n",
        "    Respond with only the setting name (2-4 words):\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_llm_response(prompt, max_length=50, temperature=0.1)\n",
        "\n",
        "    # Clean up response (remove quotes, periods, etc.)\n",
        "    response = response.strip('\"\\'.,;: ')\n",
        "\n",
        "    return response\n",
        "\n",
        "def determine_topic(title, question, answer):\n",
        "    \"\"\"Generate an appropriate topic for the content\"\"\"\n",
        "\n",
        "    # Extract meaningful content and truncate to reasonable lengths\n",
        "    title_short = title[:100] if title else \"\"\n",
        "    question_short = question[:200] if question else \"\"\n",
        "    answer_short = answer[:300] if answer else \"\"\n",
        "\n",
        "    # Create a detailed prompt with clearer instructions\n",
        "    prompt = f\"\"\"\n",
        "    Task: Analyze the content and identify the MAIN SUBJECT MATTER being discussed.\n",
        "\n",
        "    IMPORTANT: Respond ONLY with the general topic area (1-3 words).\n",
        "    - Do NOT name specific people (like \"Noam Chomsky\")\n",
        "    - Do NOT name specific books or works\n",
        "    - Focus on the BROADER FIELD or SUBJECT AREA\n",
        "\n",
        "    Examples of correct topic responses:\n",
        "    - \"Syntax\" (for discussions of linguistic structure)\n",
        "    - \"Semantics\" (for discussions about meaning in language)\n",
        "    - \"Political Philosophy\" (for discussions of political theory)\n",
        "    - \"Foreign Policy\" (for discussions about international relations)\n",
        "    - \"Media Analysis\" (for critiques of media)\n",
        "    - \"Linguistics\" (for discussions about language)\n",
        "    - \"Economic Theory\" (for discussions about economic systems or ideologies)\n",
        "    - \"Immigration Policy\" (for discussions about migration issues)\n",
        "    - \"Vietnam War\" (for discussions about this specific conflict)\n",
        "    - \"Middle East Politics\" (for discussions about politics in this region)\n",
        "    - \"Educational Theory\" (for discussions about teaching and learning)\n",
        "\n",
        "    Content to analyze:\n",
        "    TITLE: {title_short}\n",
        "    QUESTION: {question_short}\n",
        "    ANSWER EXCERPT: {answer_short}\n",
        "\n",
        "    What broad academic field or subject matter is this primarily about?\n",
        "    Respond with ONLY the general topic (1-3 words, no explanation):\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_llm_response(prompt, max_length=50, temperature=0.1)\n",
        "\n",
        "    # Clean up response\n",
        "    response = response.strip('\"\\'.,;: ')\n",
        "\n",
        "    # Additional filtering to remove person names if they still appear\n",
        "    name_indicators = [\"chomsky\", \"zinn\", \"foucault\", \"kissinger\"]\n",
        "    if any(name.lower() in response.lower() for name in name_indicators):\n",
        "        # Try to extract a topic from the title if the response contains a person's name\n",
        "        backup_prompt = f\"\"\"\n",
        "        Based only on this title: \"{title_short}\"\n",
        "\n",
        "        What general academic field or subject area (NOT a person's name) would this fall under?\n",
        "        Choose from: Linguistics, Politics, International Relations, Philosophy, Media Studies,\n",
        "        History, Economics, Sociology, Psychology, Literature, Military Studies, Education\n",
        "\n",
        "        Respond with only 1-3 words:\n",
        "        \"\"\"\n",
        "        response = get_llm_response(backup_prompt, max_length=20, temperature=0.1)\n",
        "\n",
        "    return response\n",
        "\n",
        "def determine_persona(title, question, answer):\n",
        "    \"\"\"Generate an appropriate persona for the content\"\"\"\n",
        "\n",
        "    # Extract meaningful content and truncate to reasonable lengths\n",
        "    title_short = title[:100] if title else \"\"\n",
        "    question_short = question[:200] if question else \"\"\n",
        "    answer_short = answer[:300] if answer else \"\"\n",
        "\n",
        "    # Create a detailed prompt\n",
        "    prompt = f\"\"\"\n",
        "    Task: Analyze the content and determine what type of person or professional role would typically give this kind of answer.\n",
        "\n",
        "    Examples of personas include:\n",
        "    - \"Professor\" (academic expert)\n",
        "    - \"Debater\" (skilled in formal arguments)\n",
        "    - \"Political Activist\" (advocate for political causes)\n",
        "    - \"Cognitive Scientist\" (researcher of mind and intelligence)\n",
        "    - \"Journalist\" (news reporter or analyst)\n",
        "    - \"Policy Expert\" (specialist in public policy)\n",
        "    - \"Historian\" (expert in historical events)\n",
        "    - \"Economist\" (specialist in economics)\n",
        "    - \"Political Candidate\" (person running for office)\n",
        "    - \"Cultural Critic\" (analyst of cultural trends)\n",
        "\n",
        "    Content to analyze:\n",
        "    TITLE: {title_short}\n",
        "    QUESTION: {question_short}\n",
        "    ANSWER EXCERPT: {answer_short}\n",
        "\n",
        "    Based on the tone, expertise level, perspective, and content, what persona would likely give this answer?\n",
        "    Respond with only the persona type (1-3 words):\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_llm_response(prompt, max_length=50, temperature=0.1)\n",
        "\n",
        "    # Clean up response\n",
        "    response = response.strip('\"\\'.,;: ')\n",
        "\n",
        "    return response\n",
        "\n",
        "def generate_instruction(title, setting, topic, persona):\n",
        "    \"\"\"Generate an instruction for an AI based on metadata\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Task: Create a clear, concise instruction for an AI assistant based on these parameters:\n",
        "\n",
        "    TITLE: {title[:100]}\n",
        "    SETTING: {setting}\n",
        "    TOPIC: {topic}\n",
        "    PERSONA: {persona}\n",
        "\n",
        "    Write an instruction that tells the AI to respond as if it were a {persona} in a {setting} discussing {topic}.\n",
        "    The instruction should be clear, specific, and 1-2 sentences long.\n",
        "    Begin with \"Respond as a...\" or similar directive phrase:\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_llm_response(prompt, max_length=100, temperature=0.3)\n",
        "    return response\n",
        "\n",
        "# Step 6: Process all rows to extract metadata\n",
        "print(\"\\nüîç Processing all rows to extract metadata...\")\n",
        "enriched_data = []\n",
        "\n",
        "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    # Extract raw data\n",
        "    title = str(row.get('title', ''))\n",
        "    date_str = str(row.get('date', ''))\n",
        "    question = str(row.get('question', ''))\n",
        "    answer = str(row.get('answer', ''))\n",
        "\n",
        "    # Extract year from date\n",
        "    year = extract_year(date_str)\n",
        "\n",
        "    # Generate metadata\n",
        "    setting = determine_setting(title, question, answer)\n",
        "    topic = determine_topic(title, question, answer)\n",
        "    persona = determine_persona(title, question, answer)\n",
        "    instruction = generate_instruction(title, setting, topic, persona)\n",
        "\n",
        "    # Create enriched entry\n",
        "    enriched_entry = {\n",
        "        \"year\": year,\n",
        "        \"setting\": setting,\n",
        "        \"topic\": topic,\n",
        "        \"persona\": persona,\n",
        "        \"instruction\": instruction,\n",
        "        \"input\": question,\n",
        "        \"output\": answer\n",
        "    }\n",
        "\n",
        "    enriched_data.append(enriched_entry)\n",
        "    time.sleep(0.5)  # Small delay\n",
        "\n",
        "# Step 7: Save the enriched data\n",
        "print(\"\\nüíæ Saving enriched data...\")\n",
        "\n",
        "# Save as JSON with pretty formatting\n",
        "output_file_json = \"enriched_data_before.json\"\n",
        "with open(output_file_json, 'w') as f:\n",
        "    json.dump(enriched_data, f, indent=2)\n",
        "\n",
        "# Save as CSV\n",
        "output_file_csv = \"enriched_data_before.csv\"\n",
        "enriched_df = pd.DataFrame(enriched_data)\n",
        "enriched_df.to_csv(output_file_csv, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Data saved to {output_file_json} and {output_file_csv}\")\n",
        "\n",
        "# Instead of downloading (as in Colab), inform the user where to find the files locally.\n",
        "print(\"\\nüì• The enriched data files have been saved to your current working directory.\")\n",
        "print(\"\\nüìä Sample of enriched data:\")\n",
        "print(enriched_df.head(3))\n",
        "\n",
        "print(\"\\nüéâ Processing complete! You can now use the generated JSON and CSV files from your local directory.\")\n",
        "\n",
        "# Optional: Functions to process more data if needed\n",
        "def process_additional_data(start_idx, num_rows):\n",
        "    \"\"\"Process additional rows from the dataset\"\"\"\n",
        "    print(f\"\\nüîÑ Processing rows {start_idx+1} to {min(start_idx+num_rows, len(df))}\")\n",
        "\n",
        "    additional_data = []\n",
        "    end_idx = min(start_idx + num_rows, len(df))\n",
        "\n",
        "    for i in tqdm(range(start_idx, end_idx)):\n",
        "        row = df.iloc[i]\n",
        "\n",
        "        # Extract raw data\n",
        "        title = str(row.get('title', ''))\n",
        "        date_str = str(row.get('date', ''))\n",
        "        question = str(row.get('question', ''))\n",
        "        answer = str(row.get('answer', ''))\n",
        "\n",
        "        # Extract year from date\n",
        "        year = extract_year(date_str)\n",
        "\n",
        "        # Generate metadata\n",
        "        setting = determine_setting(title, question, answer)\n",
        "        topic = determine_topic(title, question, answer)\n",
        "        persona = determine_persona(title, question, answer)\n",
        "        instruction = generate_instruction(title, setting, topic, persona)\n",
        "\n",
        "        # Create enriched entry\n",
        "        enriched_entry = {\n",
        "            \"year\": year,\n",
        "            \"setting\": setting,\n",
        "            \"topic\": topic,\n",
        "            \"persona\": persona,\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": question,\n",
        "            \"output\": answer\n",
        "        }\n",
        "\n",
        "        additional_data.append(enriched_entry)\n",
        "        time.sleep(0.5)  # Small delay\n",
        "\n",
        "    return additional_data\n",
        "\n",
        "# Uncomment the following block to process additional rows locally\n",
        "\"\"\"\n",
        "# Process next 10 rows\n",
        "more_data = process_additional_data(10, 10)\n",
        "\n",
        "# Add to existing data\n",
        "enriched_data.extend(more_data)\n",
        "\n",
        "# Save updated data\n",
        "with open(output_file_json, 'w') as f:\n",
        "    json.dump(enriched_data, f, indent=2)\n",
        "\n",
        "# Update CSV\n",
        "pd.DataFrame(enriched_data).to_csv(output_file_csv, index=False)\n",
        "print(f\"\\n‚úÖ Updated data saved to {output_file_json} and {output_file_csv}\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tczfn_pNfhgg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "unsloth_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
