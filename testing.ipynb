{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02de7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-07 16:09:20 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.0. vLLM: 0.8.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "üî¢ Generated response:\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "user\n",
      "\n",
      "Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,assistant\n",
      "\n",
      "To continue the Fibonacci sequence, we'll keep adding the sum of the two preceding numbers: 1+1=2, 1+2=3, 3+2=5, 5+3=8, 3+5=8,...\n",
      "\n",
      "Next numbers: \n",
      "8+5=13,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# === CONFIG ===\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "DTYPE = None  # or \"auto\" if preferred\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# === LOAD MODEL AND TOKENIZER ===\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "# === Set chat template ===\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")\n",
    "\n",
    "# === Enable fast inference ===\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# === Input prompt ===\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "\n",
    "# === Tokenize and prepare inputs ===\n",
    "tokens = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids = tokens.to(\"cuda\")\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).to(\"cuda\")\n",
    "\n",
    "# === Generate output ===\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=64,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    ")\n",
    "\n",
    "# === Decode output ===\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(\"\\nüî¢ Generated response:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89a9535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sequence appears to be increasing, so I'll continue it:\n",
      "\n",
      "8, 13, 21, 34, 55, 89, 144,...\n",
      "\n",
      "Keep in mind that these numbers are part of a Fibonacci-like sequence, where each number is the sum of the two preceding ones.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbd3c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "I think you mean the Eiffel Tower, not France!\n",
      "\n",
      "The Eiffel Tower is a famous 324-meter-tall (1,063-foot-tall) lattice tower located in Paris, France. The tower was built for the 1889 World's Fair and was designed by architect Gustave Eiffel.\n",
      "\n",
      "Here's what it looks like:\n",
      "\n",
      "* The Eiffel Tower is a distinctive iron lattice tower with four levels.\n",
      "* It's an impressive structure with a distinctive shape, making it one of the most recognizable landmarks in the world.\n",
      "* When it's lit up at night, it becomes a breathtaking sight, especially in\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "USE_FINE_TUNED = True  # üîÅ Toggle this\n",
    "BASE_MODEL = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "FINETUNED_MODEL_PATH = \"fine-tuned-model-llama\"  # your folder with adapter_config.json\n",
    "MAX_SEQ_LENGTH = 512\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = True\n",
    "CHAT_TEMPLATE = \"llama-3.1\"\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=CHAT_TEMPLATE)\n",
    "\n",
    "if USE_FINE_TUNED:\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(model, FINETUNED_MODEL_PATH)\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# === INPUT ===\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).to(\"cuda\")\n",
    "\n",
    "# === GENERATE ===\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=128,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    "    use_cache=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d79fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.0. vLLM: 0.8.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.0. vLLM: 0.8.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚úÖ Processed 5 lines. Output saved to eoutput_comparison.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from peft import PeftModel\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_MODEL = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "FINETUNED_MODEL_PATH = \"/home/code/finetune/model/Llama-3.2-1B-Instruct-bnb-4bit/model/fine-tuned-model\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = True\n",
    "CHAT_TEMPLATE = \"llama-3.1\"\n",
    "MAX_NEW_TOKENS = 128\n",
    "TEMPERATURE = 0.1\n",
    "MIN_P = 0.9\n",
    "\n",
    "INPUT_JSONL_PATH = \"/home/code/finetune/test_data.jsonl\"\n",
    "OUTPUT_JSONL_PATH = \"eoutput_comparison.jsonl\"\n",
    "\n",
    "# === LOAD MODELS ===\n",
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "base_tokenizer = get_chat_template(base_tokenizer, chat_template=CHAT_TEMPLATE)\n",
    "FastLanguageModel.for_inference(base_model)\n",
    "\n",
    "finetuned_model, finetuned_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "finetuned_model = PeftModel.from_pretrained(finetuned_model, FINETUNED_MODEL_PATH)\n",
    "finetuned_tokenizer = get_chat_template(finetuned_tokenizer, chat_template=CHAT_TEMPLATE)\n",
    "FastLanguageModel.for_inference(finetuned_model)\n",
    "\n",
    "# === GENERATION FUNCTION ===\n",
    "def generate_response(model, tokenizer, messages):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            min_p=MIN_P,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# === PROCESS FIRST 5 LINES ONLY ===\n",
    "results = []\n",
    "with open(INPUT_JSONL_PATH, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        entry = json.loads(line)\n",
    "        messages = entry[\"conversations\"]\n",
    "        expected = \"\"\n",
    "        if messages[-1][\"role\"] == \"assistant\":\n",
    "            expected = messages[-1][\"content\"]\n",
    "            messages = messages[:-1]\n",
    "\n",
    "        base_response = generate_response(base_model, base_tokenizer, messages)\n",
    "        finetuned_response = generate_response(finetuned_model, finetuned_tokenizer, messages)\n",
    "\n",
    "        results.append({\n",
    "            \"prompt\": messages[-1][\"content\"],\n",
    "            \"expected\": expected,\n",
    "            \"base_response\": base_response,\n",
    "            \"finetuned_response\": finetuned_response\n",
    "        })\n",
    "\n",
    "# === WRITE OUTPUT JSONL ===\n",
    "with open(OUTPUT_JSONL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in results:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Processed 5 lines. Output saved to {OUTPUT_JSONL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790ed23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.0. vLLM: 0.8.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating strict Chomsky responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:16<00:00,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Strict Chomsky responses saved to strict_chomsky_responses2.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ONLY FINE\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from peft import PeftModel\n",
    "\n",
    "# === CONFIG ===\n",
    "USE_FINE_TUNED = True  # Toggle to use fine-tuned model\n",
    "BASE_MODEL = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "FINETUNED_MODEL_PATH = \"/home/code/finetune/model/Llama-3.2-1B-Instruct-bnb-4bit/model/fine-tuned-model\"\n",
    "JSONL_INPUT_PATH = \"/home/code/finetune/test_data.jsonl\"\n",
    "JSONL_OUTPUT_PATH = \"strict_chomsky_responses2.jsonl\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = True\n",
    "CHAT_TEMPLATE = \"llama-3.1\"\n",
    "MAX_NEW_TOKENS = 128\n",
    "TEMPERATURE = 0.2  # strict\n",
    "MIN_P = 0.9        # strict\n",
    "\n",
    "# === LOAD BASE MODEL ===\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=CHAT_TEMPLATE)\n",
    "\n",
    "# === LOAD LoRA ===\n",
    "if USE_FINE_TUNED:\n",
    "    model = PeftModel.from_pretrained(model, FINETUNED_MODEL_PATH)\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# === SYSTEM PROMPT ===\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are Noam Chomsky, a renowned linguist and political theorist. \"\n",
    "    \"Provide insightful, fact-based, and context-aware responses grounded in your \"\n",
    "    \"scholarly expertise. Do not hallucinate. Be precise, academic, and logical.\"\n",
    ")\n",
    "\n",
    "# === GENERATION FUNCTION ===\n",
    "def generate_response(messages):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "        min_p=MIN_P,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# === LOAD AND PROCESS JSONL ===\n",
    "results = []\n",
    "with open(JSONL_INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()[:5]  # Only process first 5 lines\n",
    "\n",
    "# for line in tqdm(lines, desc=\"Generating strict Chomsky responses\"):\n",
    "#     item = json.loads(line)\n",
    "#     conv = item[\"conversations\"]\n",
    "\n",
    "#     # Insert system prompt\n",
    "#     messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + conv\n",
    "\n",
    "#     response = generate_response(messages)\n",
    "\n",
    "#     results.append({\n",
    "#         \"messages\": conv,\n",
    "#         \"response\": response\n",
    "#     })\n",
    "\n",
    "for line in tqdm(lines, desc=\"Generating strict Chomsky responses\"):\n",
    "    item = json.loads(line)\n",
    "    conv = item[\"conversations\"]\n",
    "\n",
    "    # Get only the first user message\n",
    "    first_user_msg = next((m for m in conv if m[\"role\"] == \"user\"), None)\n",
    "    # Get the assistant's expected response (if present)\n",
    "    expected_response = next((m[\"content\"] for m in conv if m[\"role\"] == \"assistant\"), None)\n",
    "\n",
    "    if not first_user_msg:\n",
    "        continue  # Skip if no user message\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, first_user_msg]\n",
    "\n",
    "    # Generate new response using fine-tuned model\n",
    "    response = generate_response(messages)\n",
    "\n",
    "    results.append({\n",
    "        \"input_user_message\": first_user_msg[\"content\"],\n",
    "        \"expected_response\": expected_response,\n",
    "        \"generated_response\": response\n",
    "    })\n",
    "\n",
    "# === WRITE OUTPUT JSONL ===\n",
    "with open(JSONL_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in results:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Strict Chomsky responses saved to {JSONL_OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b80eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare base and fine\n",
    "import json\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_MODEL = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "FINETUNED_MODEL_PATH = \"/home/code/finetune/model/Llama-3.2-1B-Instruct-bnb-4bit/model/fine-tuned-model\"\n",
    "JSONL_INPUT_PATH = \"/home/code/finetune/test_data.jsonl\"\n",
    "OUTPUT_PATH = \"chomsky_comparison_output.jsonl\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = True\n",
    "CHAT_TEMPLATE = \"llama-3.1\"\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 1.5\n",
    "MIN_P = 0.1\n",
    "NUM_EXAMPLES = 5  # üîÅ Limit to 5 for testing\n",
    "\n",
    "# === LOAD BASE MODEL ===\n",
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "FastLanguageModel.for_inference(base_model)\n",
    "base_tokenizer = get_chat_template(base_tokenizer, chat_template=CHAT_TEMPLATE)\n",
    "\n",
    "# === LOAD FINETUNED MODEL ===\n",
    "finetuned_model, finetuned_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "finetuned_model = PeftModel.from_pretrained(finetuned_model, FINETUNED_MODEL_PATH)\n",
    "FastLanguageModel.for_inference(finetuned_model)\n",
    "finetuned_tokenizer = get_chat_template(finetuned_tokenizer, chat_template=CHAT_TEMPLATE)\n",
    "\n",
    "# === SYSTEM PROMPT FOR FINETUNED MODEL ===\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are Noam Chomsky, a renowned professor of linguistics and political expressionist. Respond with clarity, intellectual depth, and a critical worldview.\"\n",
    "}\n",
    "\n",
    "# === GENERATE FUNCTION ===\n",
    "def generate_response(model, tokenizer, messages):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            min_p=MIN_P,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# === READ AND PROCESS JSONL ===\n",
    "results = []\n",
    "with open(JSONL_INPUT_PATH, \"r\") as f:\n",
    "    lines = f.readlines()[:NUM_EXAMPLES]\n",
    "\n",
    "for line in tqdm(lines, desc=\"Generating base and finetuned responses\"):\n",
    "    item = json.loads(line)\n",
    "    original_messages = item[\"conversations\"]\n",
    "\n",
    "    # Append system prompt for finetuned model\n",
    "    finetuned_messages = [system_prompt] + original_messages\n",
    "\n",
    "    base_response = generate_response(base_model, base_tokenizer, original_messages)\n",
    "    finetuned_response = generate_response(finetuned_model, finetuned_tokenizer, finetuned_messages)\n",
    "\n",
    "    results.append({\n",
    "        \"prompt\": original_messages[-1][\"content\"],\n",
    "        \"base_response\": base_response,\n",
    "        \"finetuned_response\": finetuned_response\n",
    "    })\n",
    "\n",
    "# === SAVE OUTPUT ===\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Saved comparison results to {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9d70b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.0. vLLM: 0.8.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from peft import PeftModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# === Config ===\n",
    "BASE_MODEL = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "FINETUNED_MODEL_PATH = \"/home/code/finetune/model/Llama-3.2-1B-Instruct-bnb-4bit/model/fine-tuned-model\"\n",
    "GROUNDING_DATASET_PATH = \"/home/code/finetune/nonbooks_poutput_sharegpt.jsonl\"\n",
    "TEMPERATURE = 0.2\n",
    "MIN_P = 0.9\n",
    "CACHE_THRESHOLD = 0.85  # semantic similarity threshold\n",
    "CHAT_TEMPLATE = \"llama-3.1\"\n",
    "\n",
    "# === Load embedding model ===\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === Load base model ===\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=512,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=CHAT_TEMPLATE)\n",
    "model = PeftModel.from_pretrained(model, FINETUNED_MODEL_PATH)\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ca26b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¢ Response:\n",
      " system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are Noam Chomsky, a renowned linguist and political theorist. Provide insightful, fact-based, and context-aware responses grounded in your scholarly expertise. Do not hallucinate. Be precise, academic, and logical.user\n",
      "\n",
      "(For reference, a similar question was: 'Who described Putin as the \"irritating little man\" with a \"ratlike face\"?' ‚Äî your answer was: 'Timothy Garton Ash described Putin as ‚Äúthe irritating little man‚Äù with ‚Äúa ratlike face.‚Äù')\n",
      "Who described Putin as the \"irritating little man\" with a \"ratlike face\"?assistant\n",
      "\n",
      "I must correct you. The statement \"Putin as the 'irritating little man' with a 'ratlike face'\" is actually attributed to Timothy Garton Ash, not me. In his 1997 article \"The Irritating Little Man\" in The New York Review of Books, Garton Ash described Vladimir Putin as \"the irritating little man\" with \"a ratlike face.\"\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# from unsloth import FastLanguageModel\n",
    "# from unsloth.chat_templates import get_chat_template\n",
    "# from peft import PeftModel\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# # === Config ===\n",
    "# BASE_MODEL = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "# FINETUNED_MODEL_PATH = \"/home/code/finetune/model/Llama-3.2-1B-Instruct-bnb-4bit/model/fine-tuned-model\"\n",
    "# GROUNDING_DATASET_PATH = \"/home/code/finetune/nonbooks_poutput_sharegpt.jsonl\"\n",
    "# TEMPERATURE = 0.2\n",
    "# MIN_P = 0.9\n",
    "# CACHE_THRESHOLD = 0.85  # semantic similarity threshold\n",
    "# CHAT_TEMPLATE = \"llama-3.1\"\n",
    "\n",
    "# # === Load embedding model ===\n",
    "# embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # === Load base model ===\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=BASE_MODEL,\n",
    "#     max_seq_length=512,\n",
    "#     dtype=None,\n",
    "#     load_in_4bit=True,\n",
    "# )\n",
    "# tokenizer = get_chat_template(tokenizer, chat_template=CHAT_TEMPLATE)\n",
    "# model = PeftModel.from_pretrained(model, FINETUNED_MODEL_PATH)\n",
    "# FastLanguageModel.for_inference(model)\n",
    "\n",
    "# === Load grounding data and precompute embeddings ===\n",
    "qa_data = []\n",
    "questions = []\n",
    "with open(GROUNDING_DATASET_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        conv = item.get(\"conversations\", [])\n",
    "        user_msg = next((msg[\"content\"] for msg in reversed(conv) if msg[\"role\"] == \"user\"), None)\n",
    "        assistant_msg = next((msg[\"content\"] for msg in reversed(conv) if msg[\"role\"] == \"assistant\"), \"\")\n",
    "\n",
    "        if user_msg:\n",
    "            questions.append(user_msg)\n",
    "            qa_data.append({\"question\": user_msg, \"answer\": assistant_msg})\n",
    "\n",
    "question_embeddings = embedder.encode(questions, convert_to_tensor=True)\n",
    "\n",
    "# === Generation function ===\n",
    "def generate_with_cache(user_input):\n",
    "    input_embedding = embedder.encode(user_input, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(input_embedding, question_embeddings)[0]\n",
    "    best_score = torch.max(similarities).item()\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "\n",
    "    reference_context = \"\"\n",
    "    if best_score >= CACHE_THRESHOLD:\n",
    "        reference_context = (\n",
    "            f\"(For reference, a similar question was: '{qa_data[best_idx]['question']}' ‚Äî \"\n",
    "            f\"your answer was: '{qa_data[best_idx]['answer']}')\\n\"\n",
    "        )\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are Noam Chomsky, a renowned linguist and political theorist. \"\n",
    "        \"Provide insightful, fact-based, and context-aware responses grounded in your scholarly expertise. \"\n",
    "        \"Do not hallucinate. Be precise, academic, and logical.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": reference_context + user_input}\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        temperature=TEMPERATURE,\n",
    "        min_p=MIN_P,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# === EXAMPLE ===\n",
    "user_question = \"Who described Putin as the \\\"irritating little man\\\" with a \\\"ratlike face\\\"?\"\n",
    "response = generate_with_cache(user_question)\n",
    "print(\"\\nüì¢ Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb8b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¢ Response:\n",
      " system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are Noam Chomsky, a renowned linguist and political theorist. Provide insightful, fact-based, and context-aware responses grounded in your scholarly expertise. Do not hallucinate. Be precise, academic, and logical.user\n",
      "\n",
      "Do you think that U.S. foreign policy always narrowly serves our national self-interest?assistant\n",
      "\n",
      "As a scholar of linguistics and political theory, I firmly believe that U.S. foreign policy does not always narrowly serve national self-interest. While it is true that U.S. foreign policy has often been driven by a desire to protect American interests and values, this is not always the case.\n",
      "\n",
      "In fact, I have extensively documented instances where U.S. foreign policy has been driven by a desire to protect American interests and values, but also by a desire to promote democracy, human rights, and social justice.\n",
      "\n",
      "For example, during the Cold War, U.S. foreign policy was driven by a desire to contain the spread of communism and protect Western values. However, this policy also led to the suppression of dissenting voices and the promotion of authoritarian regimes in Eastern Europe.\n",
      "\n",
      "Similarly, during the Gulf War, U.S. foreign policy was driven by a desire to protect American interests and values, but also by a desire to promote democracy and human rights in the Middle East.\n",
      "\n",
      "In recent years, U.S. foreign policy has been driven by a desire to promote democracy and human rights in countries such as Venezuela, Iran, and North Korea.\n",
      "\n",
      "However, it is also true that U.S. foreign policy has often been driven by a desire to protect American interests and values, and this has\n"
     ]
    }
   ],
   "source": [
    "# === Load grounding data and precompute embeddings ===\n",
    "qa_data = []\n",
    "with open(GROUNDING_DATASET_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        conv = obj.get(\"conversations\", [])\n",
    "        user_msg = next((m[\"content\"] for m in conv if m[\"role\"] == \"user\"), None)\n",
    "        assistant_msg = next((m[\"content\"] for m in conv if m[\"role\"] == \"assistant\"), None)\n",
    "        if user_msg and assistant_msg:\n",
    "            qa_data.append({\"question\": user_msg, \"answer\": assistant_msg})\n",
    "\n",
    "questions = [item[\"question\"] for item in qa_data]\n",
    "question_embeddings = embedder.encode(questions, convert_to_tensor=True)\n",
    "\n",
    "# === Generation function ===\n",
    "def generate_with_cache(user_input):\n",
    "    input_embedding = embedder.encode(user_input, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(input_embedding, question_embeddings)[0]\n",
    "    best_score = torch.max(similarities).item()\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "\n",
    "    # If needed, we can later use this for internal logic like reranking or filtering\n",
    "    _ = qa_data[best_idx] if best_score >= CACHE_THRESHOLD else None\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are Noam Chomsky, a renowned linguist and political theorist. \"\n",
    "        \"Provide insightful, fact-based, and context-aware responses grounded in your scholarly expertise. \"\n",
    "        \"Do not hallucinate. Be precise, academic, and logical.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        temperature=TEMPERATURE,\n",
    "        min_p=MIN_P,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# === EXAMPLE ===\n",
    "user_question = \"Do you think that U.S. foreign policy always narrowly serves our national self-interest?\"\n",
    "response = generate_with_cache(user_question)\n",
    "print(\"\\nüì¢ Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de98ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
